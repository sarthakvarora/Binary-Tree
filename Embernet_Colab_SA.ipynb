{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarthakvarora/Binary-Tree/blob/main/Embernet_Colab_SA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5pqBTSY1iVr",
        "outputId": "fba6a99b-7104-4835-dd3f-961e6aca9eeb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ú®üç∞‚ú® Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mamba install -q gdal=3.4.1"
      ],
      "metadata": {
        "id": "IuOUznb22BnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76e9b7a5-ae04-4d2d-93d0-49289b07238d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\u001b[1mwarning  libmamba\u001b[m Could not parse mod/etag header\n",
            "\u001b[33m\u001b[1mwarning  libmamba\u001b[m Could not parse mod/etag header\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from osgeo import gdal\n",
        "gdal.VersionInfo()"
      ],
      "metadata": {
        "id": "wMS5HfSW3Mwp",
        "outputId": "4aa534a4-db52-4cf4-b3dd-e82939b9e906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3040100'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdalinfo --version"
      ],
      "metadata": {
        "id": "uClkuidv8lQv",
        "outputId": "0c3006b8-e573-4e59-a58f-40472fd4b997",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GDAL 3.4.1, released 2021/12/27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install geojson\n",
        "!pip install folium\n",
        "!pip install geopandas\n",
        "!pip install psycopg2\n",
        "!pip install itertools\n",
        "!pip install psycopg2\n",
        "!pip install rasterio\n",
        "!pip install osr\n",
        "!pip install rasterstats\n",
        "!pip install ogr\n",
        "!pip install geoalchemy2\n",
        "!pip install uuid"
      ],
      "metadata": {
        "id": "kzHZrDC2Pg9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77fc75f2-564a-4afb-db01-9125d136d823"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: geojson in /usr/local/lib/python3.10/site-packages (3.1.0)\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.10/site-packages (0.18.0)\n",
            "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.10/site-packages (from folium) (0.8.0)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.10/site-packages (from folium) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from folium) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from folium) (2.31.0)\n",
            "Requirement already satisfied: xyzservices in /usr/local/lib/python3.10/site-packages (from folium) (2024.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2>=2.9->folium) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->folium) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->folium) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->folium) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->folium) (2024.8.30)\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.10/site-packages (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/site-packages (from geopandas) (1.26.4)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.10/site-packages (from geopandas) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from geopandas) (23.2)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.10/site-packages (from geopandas) (2.2.3)\n",
            "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.10/site-packages (from geopandas) (3.7.0)\n",
            "Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.10/site-packages (from geopandas) (2.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas>=1.4.0->geopandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas>=1.4.0->geopandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas>=1.4.0->geopandas) (2024.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/site-packages (from pyogrio>=0.7.2->geopandas) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->geopandas) (1.16.0)\n",
            "Requirement already satisfied: psycopg2 in /usr/local/lib/python3.10/site-packages (2.9.10)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement itertools (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for itertools\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: psycopg2 in /usr/local/lib/python3.10/site-packages (2.9.10)\n",
            "Requirement already satisfied: rasterio in /usr/local/lib/python3.10/site-packages (1.4.1)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.10/site-packages (from rasterio) (2.4.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/site-packages (from rasterio) (24.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/site-packages (from rasterio) (2024.8.30)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/site-packages (from rasterio) (8.1.7)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/site-packages (from rasterio) (0.7.2)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/site-packages (from rasterio) (1.26.4)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.10/site-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/site-packages (from rasterio) (3.2.0)\n",
            "Collecting osr\n",
            "  Using cached OSR-0.0.1.tar.gz (4.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting Pyglet==1.1.4 (from osr)\n",
            "  Using cached pyglet-1.1.4-py3-none-any.whl\n",
            "Collecting eyeD3>=0.6.18 (from osr)\n",
            "  Using cached eyed3-0.9.7-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting coverage<6.0.0,>=5.3.1 (from coverage[toml]<6.0.0,>=5.3.1->eyeD3>=0.6.18->osr)\n",
            "  Using cached coverage-5.5-cp310-cp310-manylinux1_x86_64.whl.metadata (8.0 kB)\n",
            "Collecting deprecation<3.0.0,>=2.1.0 (from eyeD3>=0.6.18->osr)\n",
            "  Using cached deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting filetype<2.0.0,>=1.0.7 (from eyeD3>=0.6.18->osr)\n",
            "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting toml (from coverage[toml]<6.0.0,>=5.3.1->eyeD3>=0.6.18->osr)\n",
            "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from deprecation<3.0.0,>=2.1.0->eyeD3>=0.6.18->osr) (23.2)\n",
            "Using cached eyed3-0.9.7-py3-none-any.whl (246 kB)\n",
            "Using cached coverage-5.5-cp310-cp310-manylinux1_x86_64.whl (238 kB)\n",
            "Using cached deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: osr\n",
            "  Building wheel for osr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for osr: filename=OSR-0.0.1-py3-none-any.whl size=5614 sha256=3a6fb5bec6312679c31dcea3a0c4edfb4d3a70b0157c1a768ef9938596b7e5e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/13/1f/644b668739c190d9281323e3a9f9afcd6be39e3ca86f0b3354\n",
            "Successfully built osr\n",
            "Installing collected packages: Pyglet, filetype, toml, deprecation, coverage, eyeD3, osr\n",
            "Successfully installed Pyglet-1.1.4 coverage-5.5 deprecation-2.1.0 eyeD3-0.9.7 filetype-1.2.0 osr-0.0.1 toml-0.10.2\n",
            "Collecting rasterstats\n",
            "  Using cached rasterstats-0.20.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.10/site-packages (from rasterstats) (2.4.0)\n",
            "Requirement already satisfied: click>7.1 in /usr/local/lib/python3.10/site-packages (from rasterstats) (8.1.7)\n",
            "Requirement already satisfied: cligj>=0.4 in /usr/local/lib/python3.10/site-packages (from rasterstats) (0.7.2)\n",
            "Collecting fiona (from rasterstats)\n",
            "  Using cached fiona-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.10/site-packages (from rasterstats) (1.26.4)\n",
            "Requirement already satisfied: rasterio>=1.0 in /usr/local/lib/python3.10/site-packages (from rasterstats) (1.4.1)\n",
            "Collecting simplejson (from rasterstats)\n",
            "  Using cached simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/site-packages (from rasterstats) (2.0.6)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/site-packages (from rasterio>=1.0->rasterstats) (24.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/site-packages (from rasterio>=1.0->rasterstats) (2024.8.30)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.10/site-packages (from rasterio>=1.0->rasterstats) (1.1.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/site-packages (from rasterio>=1.0->rasterstats) (3.2.0)\n",
            "Using cached rasterstats-0.20.0-py3-none-any.whl (17 kB)\n",
            "Using cached fiona-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: simplejson, fiona, rasterstats\n",
            "Successfully installed fiona-1.10.1 rasterstats-0.20.0 simplejson-3.19.3\n",
            "Collecting ogr\n",
            "  Downloading ogr-0.50.2-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/site-packages (from ogr) (42.0.2)\n",
            "Collecting deprecated (from ogr)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting gitpython (from ogr)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pygithub (from ogr)\n",
            "  Downloading PyGithub-2.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting python-gitlab (from ogr)\n",
            "  Downloading python_gitlab-4.13.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting pyyaml (from ogr)\n",
            "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from ogr) (2.31.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/site-packages (from ogr) (2.1.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/site-packages (from cryptography->ogr) (1.16.0)\n",
            "Collecting wrapt<2,>=1.10 (from deprecated->ogr)\n",
            "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython->ogr)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting pynacl>=1.4.0 (from pygithub->ogr)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n",
            "Collecting pyjwt>=2.4.0 (from pyjwt[crypto]>=2.4.0->pygithub->ogr)\n",
            "  Downloading PyJWT-2.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting typing-extensions>=4.0.0 (from pygithub->ogr)\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->ogr) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->ogr) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->ogr) (2024.8.30)\n",
            "Collecting requests (from ogr)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting requests-toolbelt>=1.0.0 (from python-gitlab->ogr)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/site-packages (from cffi>=1.12->cryptography->ogr) (2.21)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->ogr)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading ogr-0.50.2-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyGithub-2.4.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m362.6/362.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_gitlab-4.13.0-py3-none-any.whl (145 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m145.3/145.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyJWT-2.9.0-py3-none-any.whl (22 kB)\n",
            "Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wrapt, typing-extensions, smmap, requests, pyyaml, pyjwt, requests-toolbelt, pynacl, gitdb, deprecated, python-gitlab, gitpython, pygithub, ogr\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "Successfully installed deprecated-1.2.14 gitdb-4.0.11 gitpython-3.1.43 ogr-0.50.2 pygithub-2.4.0 pyjwt-2.9.0 pynacl-1.5.0 python-gitlab-4.13.0 pyyaml-6.0.2 requests-2.32.3 requests-toolbelt-1.0.0 smmap-5.0.1 typing-extensions-4.12.2 wrapt-1.16.0\n",
            "Collecting geoalchemy2\n",
            "  Downloading GeoAlchemy2-0.15.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting SQLAlchemy>=1.4 (from geoalchemy2)\n",
            "  Downloading SQLAlchemy-2.0.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from geoalchemy2) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/site-packages (from SQLAlchemy>=1.4->geoalchemy2) (4.12.2)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.4->geoalchemy2)\n",
            "  Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Downloading GeoAlchemy2-0.15.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m599.5/599.5 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: greenlet, SQLAlchemy, geoalchemy2\n",
            "Successfully installed SQLAlchemy-2.0.36 geoalchemy2-0.15.2 greenlet-3.1.1\n",
            "Collecting uuid\n",
            "  Downloading uuid-1.30.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: uuid\n",
            "  Building wheel for uuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uuid: filename=uuid-1.30-py3-none-any.whl size=6479 sha256=63241029013fadc50ceb148a3c90ae5c1888bb5535bd37b6620804028455b50f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/08/9e/f0a977dfe55051a07e21af89200125d65f1efa60cbac61ed88\n",
            "Successfully built uuid\n",
            "Installing collected packages: uuid\n",
            "Successfully installed uuid-1.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas\n",
        "import geopandas as gpd\n",
        "import requests\n",
        "import geojson\n",
        "import folium\n",
        "\n",
        "import psycopg2\n",
        "import psycopg2.extras\n",
        "import numpy as np\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "import rasterio.mask\n",
        "from osgeo import gdal\n",
        "import math\n",
        "import geoalchemy2\n",
        "import ogr\n",
        "# import osr\n",
        "import uuid\n",
        "import os\n",
        "# import rasterstats\n",
        "from collections import Counter\n",
        "from joblib import Parallel, delayed\n",
        "from scipy.stats import pareto\n",
        "from shapely.ops import cascaded_union\n",
        "from sqlalchemy import create_engine\n",
        "from rasterstats import zonal_stats\n",
        "from shapely import wkt\n",
        "from shapely.geometry import LineString\n",
        "from shapely.geometry import Point\n",
        "from shapely.geometry import MultiPolygon\n",
        "from shapely.geometry import Polygon\n",
        "from shapely.wkt import loads\n",
        "from sqlalchemy import text\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "WfPoE2DRc1Q_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive',force_remount = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmv9W9oTWL8-",
        "outputId": "9b7451e5-6725-4090-8c2e-030f8a74afb4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare the pareto distribution data\n",
        "\n",
        "vegetation_dt = geopandas.read_file('/content/drive/MyDrive/embernet/harmonized_csv/vegetation_I.csv')\n",
        "structure_dt = geopandas.read_file('/content/drive/MyDrive/embernet/harmonized_csv/components.csv')\n",
        "assembly_dt = geopandas.read_file('/content/drive/MyDrive/embernet/harmonized_csv/assembly.csv')\n"
      ],
      "metadata": {
        "id": "Exz-BdlacWoH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ignition_dt = geopandas.read_file('/content/drive/MyDrive/embernet/IBHS_data/ignition_data.csv')"
      ],
      "metadata": {
        "id": "PZbgosJUHo6h"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "veg_pareto_dt = pd.DataFrame(columns=['wind_speed', 'subs','vec_len', 'b','ploc','scale'])\n",
        "for j, k in itertools.product(['Idle','Medium','High'],['Grass','Shrub','Tree']):\n",
        "    pareto_vec = vegetation_dt[(vegetation_dt['vegetation_type']==k) & (vegetation_dt['wind_speed'] ==j)]['total_flying_distance'].values\n",
        "    # print(pareto_vec)\n",
        "    pareto_vec = [float(i) for i in pareto_vec]\n",
        "    b_samp, loc_samp, scale_samp = pareto.fit(pareto_vec)\n",
        "    n_emb = len(pareto_vec)\n",
        "    out_dt =[j,k,n_emb,b_samp,loc_samp,scale_samp]\n",
        "    # veg_pareto_dt = veg_pareto_dt.append(out_dt)\n",
        "    veg_pareto_dt = pd.concat([veg_pareto_dt,pd.DataFrame(columns=veg_pareto_dt.columns,data=(out_dt,),)])\n",
        "\n",
        "\n",
        "# # print(vegetation_dt)\n",
        "str_pareto_dt = pd.DataFrame(columns=['wind_speed','assembly','material','vec_len', 'b','ploc','scale'])\n",
        "\n",
        "str_simple_dt = structure_dt[['wind_speed','assembly','material']].drop_duplicates()\n",
        "# print()\n",
        "for i in range(len(str_simple_dt)):\n",
        "    samp_dt = str_simple_dt.iloc[[i]]\n",
        "    # print(samp_dt)\n",
        "    j=samp_dt['wind_speed'].values[0]\n",
        "    k=samp_dt['assembly'].values[0]\n",
        "    l=samp_dt['material'].values[0]\n",
        "    # print([j,k,l])\n",
        "    pareto_dt = structure_dt[(structure_dt['wind_speed']==j) &(structure_dt['assembly']==k) & (structure_dt['material']==l)]\n",
        "    pareto_vec = pareto_dt['total_flying_distance'].values\n",
        "    pareto_vec = [float(m) for m in pareto_vec]\n",
        "    n_emb = len(pareto_vec)\n",
        "    b_samp, loc_samp, scale_samp = pareto.fit(pareto_vec)\n",
        "    out_dt =[j,k,l,n_emb,b_samp,loc_samp,scale_samp]\n",
        "    # veg_pareto_dt = veg_pareto_dt.append(out_dt)\n",
        "    str_pareto_dt = pd.concat([str_pareto_dt,pd.DataFrame(columns=str_pareto_dt.columns,data=(out_dt,),)])\n",
        "\n",
        "\n",
        "asm_pareto_dt = pd.DataFrame(columns=['wind_speed','cladding','wall_type','material','roof','vec_len', 'b','ploc','scale'])\n",
        "\n",
        "asm_simple_dt = assembly_dt[['wind_speed','cladding','wall_type','material','roof']].drop_duplicates()\n",
        "# cladding \twall_type \tmaterial \troof\n",
        "for i in range(len(str_simple_dt)):\n",
        "    samp_dt = asm_simple_dt.iloc[[i]]\n",
        "    j=samp_dt['wind_speed'].values[0]\n",
        "    k=samp_dt['cladding'].values[0]\n",
        "    l=samp_dt['wall_type'].values[0]\n",
        "    m=samp_dt['material'].values[0]\n",
        "    n=samp_dt['roof'].values[0]\n",
        "    # print([j,k,l])\n",
        "    pareto_dt = assembly_dt[(assembly_dt['wind_speed']==j) &(assembly_dt['cladding']==k) & (assembly_dt['wall_type']==l)& (assembly_dt['material']==m)& (assembly_dt['roof']==n)]\n",
        "    pareto_vec = pareto_dt['total_flying_distance'].values\n",
        "    # print(pareto_vec)\n",
        "    pareto_vec = [float(o) for o in pareto_vec]\n",
        "\n",
        "    b_samp, loc_samp, scale_samp = pareto.fit(pareto_vec)\n",
        "    n_emb = len(pareto_vec)\n",
        "    out_dt =[j,k,l,m,n,n_emb,b_samp,loc_samp,scale_samp]\n",
        "    # veg_pareto_dt = veg_pareto_dt.append(out_dt)\n",
        "    asm_pareto_dt = pd.concat([asm_pareto_dt,pd.DataFrame(columns=asm_pareto_dt.columns,data=(out_dt,),)])"
      ],
      "metadata": {
        "id": "txpSzfesdqPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d5b8c4-bab8-409e-e968-4b1bc06fed0d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-1c0225e0f5cb>:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  veg_pareto_dt = pd.concat([veg_pareto_dt,pd.DataFrame(columns=veg_pareto_dt.columns,data=(out_dt,),)])\n",
            "<ipython-input-10-1c0225e0f5cb>:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  str_pareto_dt = pd.concat([str_pareto_dt,pd.DataFrame(columns=str_pareto_dt.columns,data=(out_dt,),)])\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_continuous_distns.py:7866: RuntimeWarning: divide by zero encountered in scalar divide\n",
            "  return ndata / np.sum(np.log((data - location) / scale))\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_continuous_distns.py:7889: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  return dL_dLocation(shape, location) - dL_dScale(shape, scale)\n",
            "<ipython-input-10-1c0225e0f5cb>:56: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  asm_pareto_dt = pd.concat([asm_pareto_dt,pd.DataFrame(columns=asm_pareto_dt.columns,data=(out_dt,),)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_parcel_geoms(parno,county = 'Sonoma'):\n",
        "    ## Set up access controlls\n",
        "    #Todo: should this be some kind of 'secret' or be stored in a secrets thing?\n",
        "    #Todo: check database to see if a parcel has been queried recently or if we have current enough data\n",
        "    # and returned that analysis instead of buying another parcel and analyisin\n",
        "    db_cred = {'user': 'jifesypi',\n",
        "                'password': 'Qxds23zjkpIIU343-GGHNlxqFdD3Pdlr',\n",
        "                'host': 'mild-williams-pear.db.elephantsql.com',\n",
        "                'port': '5432',\n",
        "                'database': 'jifesypi',\n",
        "                'epsg': '4326',\n",
        "                'schema':'schema'\n",
        "                }\n",
        "\n",
        "    #Create connection to the database\n",
        "    engine = create_engine(f\"postgresql://{db_cred['user']}:{db_cred['password']}@{db_cred['host']}:{db_cred['port']}/{db_cred['database']}\")\n",
        "\n",
        "    sql_query_parcels = f\"SELECT * FROM public.california_parcels WHERE parno = '{parno}' AND county = '{county}'\"\n",
        "\n",
        "    parcel_dataframe = geopandas.read_postgis(sql_query_parcels, engine)\n",
        "\n",
        "    #Set parcel number\n",
        "    # parcel_dataframe= sonoma_parcel_dataframe[sonoma_parcel_dataframe['parno'] == parno]\n",
        "\n",
        "\n",
        "    print(parcel_dataframe)\n",
        "    # Query postgres database based on parcel geometry\n",
        "    #Set query EPSG (WSG 4326)\n",
        "    parcel_dataframe = parcel_dataframe.set_crs('EPSG:4326')\n",
        "\n",
        "    #Convert to 26910, stateplane & meters\n",
        "    parcel_dataframe_26910 = parcel_dataframe.to_crs(26910)\n",
        "\n",
        "    #Buffer parcel 1000 meters\n",
        "    buffer_parcel_dataframe_26910 = parcel_dataframe_26910.buffer(1000).to_crs(4326)\n",
        "\n",
        "    #Extract well known text\n",
        "    buffer_parcel_geometry_wkt = buffer_parcel_dataframe_26910.geometry.to_wkt().to_list()[0]\n",
        "    parcel_geometry_wkt = parcel_dataframe_26910.to_crs(4326).geometry.to_wkt().to_list()[0]\n",
        "\n",
        "    #Query for neighborhood of parcels\n",
        "    sql_query_parcels = f\"SELECT * FROM public.california_parcels WHERE ST_INTERSECTS(geom, ST_BUFFER('SRID=4326;{buffer_parcel_geometry_wkt}', 0))\"\n",
        "\n",
        "    #Send to engine\n",
        "    neighborhood_parcels = geopandas.read_postgis(sql_query_parcels, engine)\n",
        "\n",
        "\n",
        "    #Need to union to send to wkt query\n",
        "    # neighborhood_parcels_union = geopandas.GeoSeries(unary_union(neighborhood_parcels.geometry.values))\n",
        "    neighborhood_parcels_union = neighborhood_parcels.unary_union\n",
        "\n",
        "    #Get wkt of neighborhood of parcels\n",
        "    neighborhood_geometry_wkt = neighborhood_parcels_union.wkt\n",
        "\n",
        "    #Query for extended neighborhood (1000m) of structures\n",
        "    sql_query_parcels = f\"SELECT * FROM public.california_structures WHERE ST_INTERSECTS(geom, ST_BUFFER('SRID=4326;{neighborhood_geometry_wkt}',0))\"\n",
        "\n",
        "    neighborhood_structures = geopandas.read_postgis(sql_query_parcels, engine)\n",
        "\n",
        "    #Query for parcel structures\n",
        "    sql_query_structures = f\"SELECT * FROM public.california_structures WHERE ST_INTERSECTS(geom, ST_BUFFER('SRID=4326;{parcel_geometry_wkt}',0))\"\n",
        "\n",
        "    parcel_structures = geopandas.read_postgis(sql_query_structures, engine)\n",
        "    engine.dispose()\n",
        "    return parcel_dataframe, neighborhood_parcels, parcel_structures, neighborhood_structures"
      ],
      "metadata": {
        "id": "3yPlDnPFlNuH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_vegetation_neighborhood(neighborhood_parcels,county = 'Sonoma', do_all = True, update=True ,return_gpd = False):\n",
        "    filename = str(uuid.uuid4())\n",
        "\n",
        "    db_cred = {'user': 'jifesypi',\n",
        "            'password': 'Qxds23zjkpIIU343-GGHNlxqFdD3Pdlr',\n",
        "            'host': 'mild-williams-pear.db.elephantsql.com',\n",
        "            'port': '5432',\n",
        "            'database': 'jifesypi',\n",
        "            'epsg': '4326',\n",
        "            'schema':'schema'\n",
        "            }\n",
        "\n",
        "    #Create connection to the database\n",
        "    engine = create_engine(f\"postgresql://{db_cred['user']}:{db_cred['password']}@{db_cred['host']}:{db_cred['port']}/{db_cred['database']}\")\n",
        "\n",
        "    parno_values = ','.join([f\"'{parno}'\" for parno in neighborhood_parcels['parno']])\n",
        "\n",
        "    # print(parno_values)\n",
        "\n",
        "    # Construct the SQL query\n",
        "    sql_query_parcels = f\"SELECT * FROM public.parcel_veg WHERE parno IN ({parno_values}) AND county = '{county}'\"\n",
        "\n",
        "    previously_completed = geopandas.read_postgis(sql_query_parcels, engine)\n",
        "\n",
        "    if do_all== False:\n",
        "        neighborhood_parcels = neighborhood_parcels[~neighborhood_parcels['parno'].isin(previously_completed['parno'])]\n",
        "\n",
        "    if neighborhood_parcels.shape[0] >0:\n",
        "        # Need to union to send to wkt query\n",
        "        neighborhood_parcels_union = neighborhood_parcels.unary_union\n",
        "\n",
        "        #Get wkt of neighborhood of parcels\n",
        "        neighborhood_geometry_wkt = neighborhood_parcels_union.wkt\n",
        "\n",
        "        #Query for extended neighborhood (1000m) of structures\n",
        "        sql_query_parcels = f\"SELECT * FROM public.veg_map WHERE ST_INTERSECTS(geom, ST_BUFFER('SRID=4326;{neighborhood_geometry_wkt}',0))\"\n",
        "\n",
        "        neighborhood_veg = geopandas.read_postgis(sql_query_parcels, engine)\n",
        "        engine.dispose()\n",
        "\n",
        "        with rasterio.open(\"/content/Stack_Sonoma_2021_biomass.tif\") as src:\n",
        "            biomass_out_image, biomass_out_transform = rasterio.mask.mask(src, neighborhood_parcels.geom, crop=True)\n",
        "            biomass_out_meta = src.meta\n",
        "\n",
        "        biomass_out_meta.update({\"driver\": \"GTiff\",\n",
        "                        \"height\": biomass_out_image.shape[1],\n",
        "                        \"width\": biomass_out_image.shape[2],\n",
        "                        \"transform\": biomass_out_transform})\n",
        "\n",
        "        with rasterio.open(\"/content/processing_folder/\"+filename+\"_biomass.tif\", \"w\", **biomass_out_meta) as dest:\n",
        "            dest.write(biomass_out_image)\n",
        "        # from osgeo import gdal, ogr, osr\n",
        "\n",
        "        in_path = '/content/processing_folder/'+filename+'_class.tif'\n",
        "\n",
        "        out_path = '/content/processing_folder/'+filename+'_class.gpkg'\n",
        "\n",
        "        vegetation_dataframe = gpd.overlay(neighborhood_parcels, neighborhood_veg, how='intersection')\n",
        "\n",
        "        vegetation_dataframe[['Class_str']] =  vegetation_dataframe[['LF_FOREST']].values\n",
        "        d = {\n",
        "            'Agriculture' : 'Grass' ,\n",
        "            'Aquatic Vegetation' : 'Shrub',\n",
        "            'Barren and Sparsely Vegetated' : 'Grass',\n",
        "            'Conifer Forest' : 'Tree',\n",
        "            'Developed' : 'Grass',\n",
        "            'Forest Sliver' : 'Tree',\n",
        "            'Hardwood Forest' : 'Tree',\n",
        "            'Herbaceous': 'Grass',\n",
        "            'Herbaceous Wetland' : 'Tree',\n",
        "            'Mixed Conifer-Hardwood Forest' : 'Tree',\n",
        "            'Non-native Forest' : 'Tree',\n",
        "            'Non-native Shrub' : 'Shrub',\n",
        "            'Riparian Forest' : 'Tree',\n",
        "            'Riparian Shrub' : 'Shrub',\n",
        "            'Salt Marsh' : 'Grass',\n",
        "            'Shrub' : 'Shrub',\n",
        "            'Water' : 'Grass'}\n",
        "        vegetation_dataframe.Class_str = vegetation_dataframe.Class_str.map(d)\n",
        "\n",
        "        vegetation_dataframe['biomass'] = zonal_stats(vegetation_dataframe,\n",
        "                                \"/content/processing_folder/\"+filename+\"_biomass.tif\",\n",
        "                                stats=\"mean\", # any other stat will also do\n",
        "                                categorical=False,\n",
        "                                nodata = np.nan)\n",
        "        vegetation_dataframe['biomass'] = [i['mean'] for i in vegetation_dataframe['biomass']]\n",
        "        vegetation_dataframe['area'] = vegetation_dataframe.to_crs(6414).area/10000\n",
        "        vegetation_dataframe['MG'] = vegetation_dataframe['area']*(vegetation_dataframe['biomass'])\n",
        "        emb_vec = []\n",
        "        print(vegetation_dataframe)\n",
        "\n",
        "        # vegetation_dataframe['Class_str'] = np.nan_to_num(vegetation_dataframe['Class_str'], nan='Grass')\n",
        "        vegetation_dataframe['Class_str'] = vegetation_dataframe['Class_str'].fillna('Grass')\n",
        "        print(vegetation_dataframe)\n",
        "        for i in range(len(vegetation_dataframe)):\n",
        "            iloc_dt = vegetation_dataframe.iloc[[i]]\n",
        "            print(iloc_dt['Class_str'].values[0])\n",
        "            emb_vec.append(veg_pareto_dt[(veg_pareto_dt.subs == iloc_dt['Class_str'].values[0] ) &(veg_pareto_dt.Wind_Speed == 'High')].vec_len.values[0])\n",
        "        vegetation_dataframe['embers_base'] = emb_vec\n",
        "        #The division by five is for the assumed kg of vegetation used in the development of the distributions\n",
        "        #The multiplication is kg -> MG\n",
        "        vegetation_dataframe['embers'] = vegetation_dataframe['MG']*vegetation_dataframe['embers_base']*1000/5\n",
        "        vegetation_dataframe = vegetation_dataframe.fillna(0)\n",
        "        os.remove(\"/content/processing_folder/\"+filename+\"_biomass.tif\")\n",
        "\n",
        "\n",
        "        if update:\n",
        "            # Start a transaction\n",
        "            with engine.begin() as conn:\n",
        "                # Remove rows with existing 'parno' values\n",
        "                existing_parnos = vegetation_dataframe['parno'].unique()\n",
        "                # print(existing_parnos)\n",
        "                existing_parnos_str = ','.join(f\"'{parno}'\" for parno in existing_parnos) # surround each parno with quotes and join them with commas\n",
        "                print(existing_parnos_str)\n",
        "                delete_query = f\"DELETE FROM parcel_veg WHERE parno IN ({existing_parnos_str})\"\n",
        "                conn.execute(delete_query)\n",
        "\n",
        "\n",
        "\n",
        "            vegetation_dataframe[\"geom\"] = vegetation_dataframe[\"geometry\"]\n",
        "\n",
        "            vegetation_dataframe[\"geom\"] = [MultiPolygon([feature]) if isinstance(feature, Polygon) else feature for feature in vegetation_dataframe[\"geom\"]]\n",
        "\n",
        "            vegetation_dataframe.set_geometry(\"geom\")\n",
        "            vegetation_dataframe = vegetation_dataframe.drop('geometry', axis=1)\n",
        "\n",
        "            # zones = zones.loc[zones.geometry.geometry.type=='MultiPolygon']\n",
        "            vegetation_dataframe['geom'] = vegetation_dataframe['geom'].apply(lambda x: geoalchemy2.WKTElement(x.wkt, srid=4326))\n",
        "\n",
        "            vegetation_dataframe.to_sql('parcel_veg', engine, if_exists='append', index=False,\n",
        "                            dtype={'geom': geoalchemy2.Geometry('MultiPolygon', srid= 4326)})\n",
        "            engine.dispose()\n",
        "\n",
        "    if return_gpd:\n",
        "        #Query for parcel structures\n",
        "        sql_query_parcels = f\"SELECT * FROM public.parcel_veg WHERE parno IN ({parno_values}) AND county = '{county}'\"\n",
        "        vegetation_dataframe = geopandas.read_postgis(sql_query_parcels, engine)\n",
        "        engine.dispose()\n",
        "        return vegetation_dataframe"
      ],
      "metadata": {
        "id": "JSUuMvUvkRpB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_structure_pareto(parcel_structures):\n",
        "    n_embers = str_pareto_dt[(str_pareto_dt.assembly == 'C' )&(str_pareto_dt.material == 'CO' ) &(str_pareto_dt.wind_speed == 'High')].vec_len[0]\n",
        "    parcel_structures['embers'] = n_embers*parcel_structures.to_crs(6414).area*6\n",
        "    return(parcel_structures)"
      ],
      "metadata": {
        "id": "tOmlHIIqkmIx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def structure_zone_gen(i_samp,structure_dataframe):\n",
        "    iloc_new_dt = structure_dataframe.iloc[[i_samp]]\n",
        "    b_ = str_pareto_dt[(str_pareto_dt.assembly == 'C' )&(str_pareto_dt.material == 'CO' ) &(str_pareto_dt.wind_speed == 'High')].b.values[0]\n",
        "    loc_ = str_pareto_dt[(str_pareto_dt.assembly == 'C' )&(str_pareto_dt.material == 'CO' ) &(str_pareto_dt.wind_speed == 'High')].ploc.values[0]\n",
        "    scale_ = str_pareto_dt[(str_pareto_dt.assembly == 'C' )&(str_pareto_dt.material == 'CO' ) &(str_pareto_dt.wind_speed == 'High')].scale.values[0]\n",
        "\n",
        "    emb_count =  int(max(iloc_new_dt['embers'].values[0],75000))\n",
        "\n",
        "    scale = 1\n",
        "\n",
        "    if emb_count > 1000000:\n",
        "        scale = emb_count/1000000\n",
        "        emb_count = 1000000\n",
        "\n",
        "    emb_dist = pareto.rvs(size=emb_count,b=b_,loc=loc_,scale=scale_)\n",
        "\n",
        "    emb_dist = [int(math.ceil(i)) for i in emb_dist]\n",
        "\n",
        "\n",
        "    values, counts = np.unique(emb_dist, return_counts=True)\n",
        "\n",
        "    counts =  counts*scale\n",
        "\n",
        "    obj_r = math.sqrt(iloc_new_dt.to_crs(6414).area/3.14)\n",
        "\n",
        "    values_area = (3.14*(obj_r+values)*(obj_r+values)) - (3.14*(obj_r+values -1)*(obj_r+values -1))\n",
        "\n",
        "    counts_area = counts/values_area\n",
        "\n",
        "    max_v_vec = values <= max(values[counts_area > min(1,max(counts_area-1))])\n",
        "\n",
        "    values = values[max_v_vec]\n",
        "\n",
        "    counts = counts[max_v_vec]\n",
        "\n",
        "    counts_area = counts_area[max_v_vec]\n",
        "\n",
        "    #This is already a short cut. I'd rather not short cut any more than k == 15.\n",
        "    k = 30\n",
        "    l = (len(counts)-1)\n",
        "    k_vec =  np.sort([i for i in range(min(5,len(counts)))]+[l]+[np.ceil(l*(1/(i+1))) for i in range(k)][::-1])\n",
        "    k_vec = np.sort(list(set(k_vec)))\n",
        "    k_vec = [int(i) for i in k_vec]\n",
        "    norm_count_vec = (np.cumsum(counts)[k_vec])/np.sum(counts)\n",
        "    norm_0_count_vec = np.insert(norm_count_vec, 0, 0)[0:len(norm_count_vec)]\n",
        "    counts_vec = np.sum(counts)*(norm_count_vec-norm_0_count_vec)\n",
        "    values_vec = values[k_vec]\n",
        "\n",
        "    counts, values = counts_vec,values_vec\n",
        "\n",
        "    xval_low = np.insert(values, 0, 0)[0:(len(values))]\n",
        "    xval_high = np.insert(values, 0, 0)[1:(len(values)+1)]\n",
        "\n",
        "    # print(xval_low)\n",
        "    # print(xval_high)\n",
        "    def zone_out(xval_low,xval_high):\n",
        "        zone_low = iloc_new_dt.geometry.buffer(xval_low).unary_union\n",
        "        zone_high = iloc_new_dt.geometry.buffer(xval_high).unary_union\n",
        "        return zone_high.difference(zone_low)\n",
        "    iloc_new_dt = iloc_new_dt.to_crs(6414)\n",
        "\n",
        "    zone_vec = np.vectorize(zone_out)(xval_low,xval_high)\n",
        "\n",
        "    zone_vec = np.append(iloc_new_dt.geometry.to_crs(6414),zone_vec)\n",
        "    counts = np.append(counts[0],counts)\n",
        "    values = np.append(values[0],values)\n",
        "\n",
        "    zones = geopandas.GeoDataFrame()\n",
        "    # zones.is_copy = False\n",
        "\n",
        "    zones['counts'] = counts\n",
        "    zones['values'] = values\n",
        "    zones['values'][0] = 0\n",
        "    zones.geometry = geopandas.GeoSeries(zone_vec)\n",
        "    zones['area_r'] =  zones.geometry.area\n",
        "    counts_area_r_vec = zones['counts']/zones['area_r']\n",
        "    counts_area_r_vec[0] = counts_area_r_vec[1]\n",
        "    zones['counts_area_r'] = counts_area_r_vec\n",
        "    # zones['counts_area_r'][0] = zones['counts_area_r'][1]\n",
        "    # zones['class'] = np.repeat(iloc_new_dt['Class_str'][0],len(zones['counts_area_r']))\n",
        "    zones['Class_str'] = 'Structure'\n",
        "    zones['Class'] = 15\n",
        "    iloc_new_dt['area'] = iloc_new_dt.area\n",
        "\n",
        "    zones['area'] =  iloc_new_dt['area'].iloc[0]\n",
        "\n",
        "    zones = zones.set_crs('EPSG:6414')\n",
        "    zones = zones.to_crs(4326)\n",
        "    return zones"
      ],
      "metadata": {
        "id": "fBpP4RTfkpIE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def veg_zone_gen(i_samp,vegetation_dataframe):\n",
        "    iloc_new_dt = vegetation_dataframe.iloc[[i_samp]]\n",
        "    b_ = veg_pareto_dt[(veg_pareto_dt.subs == iloc_new_dt['Class_str'].values[0] ) &(veg_pareto_dt.wind_speed == 'High')].b.values[0]\n",
        "    loc_ = veg_pareto_dt[(veg_pareto_dt.subs == iloc_new_dt['Class_str'].values[0] ) &(veg_pareto_dt.wind_speed == 'High')].ploc.values[0]\n",
        "    scale_ = veg_pareto_dt[(veg_pareto_dt.subs == iloc_new_dt['Class_str'].values[0] ) &(veg_pareto_dt.wind_speed == 'High')].scale.values[0]\n",
        "\n",
        "    emb_count =  int(max(iloc_new_dt['embers'].values[0],75000))\n",
        "\n",
        "    scale = 1\n",
        "\n",
        "    if emb_count > 1000000:\n",
        "        scale = emb_count/1000000\n",
        "        emb_count = 1000000\n",
        "\n",
        "    emb_dist = pareto.rvs(size=emb_count,b=b_,loc=loc_,scale=scale_)\n",
        "\n",
        "    emb_dist = [int(math.ceil(i)) for i in emb_dist]\n",
        "\n",
        "\n",
        "    values, counts = np.unique(emb_dist, return_counts=True)\n",
        "\n",
        "    counts =  counts*scale\n",
        "\n",
        "    obj_r = math.sqrt(iloc_new_dt.to_crs(6414).area/3.14)\n",
        "\n",
        "    values_area = (3.14*(obj_r+values)*(obj_r+values)) - (3.14*(obj_r+values -1)*(obj_r+values -1))\n",
        "\n",
        "    counts_area = counts/values_area\n",
        "\n",
        "    max_v_vec = values <= max(values[counts_area > min(1,max(counts_area-1))])\n",
        "\n",
        "    values = values[max_v_vec]\n",
        "\n",
        "    counts = counts[max_v_vec]\n",
        "\n",
        "    counts_area = counts_area[max_v_vec]\n",
        "\n",
        "    #This is already a short cut. I'd rather not short cut any more than k == 15.\n",
        "    k = 30\n",
        "    l = (len(counts)-1)\n",
        "    k_vec =  np.sort([i for i in range(min(5,len(counts)))]+[l]+[np.ceil(l*(1/(i+1))) for i in range(k)][::-1])\n",
        "    k_vec = np.sort(list(set(k_vec)))\n",
        "    k_vec = [int(i) for i in k_vec]\n",
        "    norm_count_vec = (np.cumsum(counts)[k_vec])/np.sum(counts)\n",
        "    norm_0_count_vec = np.insert(norm_count_vec, 0, 0)[0:len(norm_count_vec)]\n",
        "    counts_vec = np.sum(counts)*(norm_count_vec-norm_0_count_vec)\n",
        "    values_vec = values[k_vec]\n",
        "\n",
        "    counts, values = counts_vec,values_vec\n",
        "\n",
        "    xval_low = np.insert(values, 0, 0)[0:(len(values))]\n",
        "    xval_high = np.insert(values, 0, 0)[1:(len(values)+1)]\n",
        "\n",
        "    # print(xval_low)\n",
        "    # print(xval_high)\n",
        "    def zone_out(xval_low,xval_high):\n",
        "        zone_low = iloc_new_dt.geometry.buffer(xval_low).unary_union\n",
        "        zone_high = iloc_new_dt.geometry.buffer(xval_high).unary_union\n",
        "        return zone_high.difference(zone_low)\n",
        "    iloc_new_dt = iloc_new_dt.to_crs(6414)\n",
        "    zone_vec = np.vectorize(zone_out)(xval_low,xval_high)\n",
        "\n",
        "    zone_vec = np.append(iloc_new_dt.geometry.to_crs(6414),zone_vec)\n",
        "    counts = np.append(counts[0],counts)\n",
        "    values = np.append(values[0],values)\n",
        "\n",
        "    zones = geopandas.GeoDataFrame()\n",
        "    # zones.is_copy = False\n",
        "\n",
        "    zones['counts'] = counts\n",
        "    zones['values'] = values\n",
        "    zones['values'][0] = 0\n",
        "    # print(values)\n",
        "    # print(xval_low)\n",
        "    # zones['low'] = xval_low[0:(len(values))]\n",
        "    # zones['high'] = xval_high[0:(len(values))]\n",
        "\n",
        "\n",
        "    zones.geometry = geopandas.GeoSeries(zone_vec)\n",
        "    zones['area_r'] =  zones.geometry.area\n",
        "    counts_area_r_vec = zones['counts']/zones['area_r']\n",
        "    counts_area_r_vec[0] = counts_area_r_vec[1]\n",
        "    zones['counts_area_r'] = counts_area_r_vec\n",
        "    # zones['counts_area_r'][0] = zones['counts_area_r'][1]\n",
        "    # zones['class'] = np.repeat(iloc_new_dt['Class_str'][0],len(zones['counts_area_r']))\n",
        "\n",
        "    iloc_new_dt['area'] = iloc_new_dt.area\n",
        "    zones['area'] = iloc_new_dt['area'].iloc[0]\n",
        "    zones['Class_str'] = iloc_new_dt['Class_str'].iloc[0]\n",
        "    zones['LF_FOREST'] = iloc_new_dt['LF_FOREST'].iloc[0]\n",
        "    zones['MG'] = iloc_new_dt['MG'].iloc[0]\n",
        "\n",
        "    zones = zones.set_crs('EPSG:6414')\n",
        "    zones = zones.to_crs(4326)\n",
        "    return zones"
      ],
      "metadata": {
        "id": "fD8J_0LikuFp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_network_kpartite(PARNO = '182-390-032', do_all = True, update=True ,return_gpd = False):\n",
        "    db_cred = {'user': 'jifesypi',\n",
        "            'password': 'Qxds23zjkpIIU343-GGHNlxqFdD3Pdlr',\n",
        "            'host': 'mild-williams-pear.db.elephantsql.com',\n",
        "            'port': '5432',\n",
        "            'database': 'jifesypi',\n",
        "            'epsg': '4326',\n",
        "            'schema':'schema'\n",
        "            }\n",
        "    engine = create_engine(f\"postgresql://{db_cred['user']}:{db_cred['password']}@{db_cred['host']}:{db_cred['port']}/{db_cred['database']}\")\n",
        "    # sql_query_parcels = f\"SELECT * FROM public.embernet_kpartite WHERE parno_1 = '{PARNO}'\"\n",
        "    # sample_summary = geopandas.read_postgis(sql_query_parcels, engine)\n",
        "    # engine.dispose()\n",
        "    # # sample_summary = []\n",
        "    # does_not_exist = len(sample_summary) == 0\n",
        "    # print(does_not_exist)\n",
        "    # if len(sample_summary) > 0:\n",
        "    #     sample_summary['geom'] = sample_summary['geom'].apply(lambda x: geoalchemy2.WKTElement(x.wkt, srid=4326))\n",
        "\n",
        "    if (do_all or does_not_exist):\n",
        "        parcel_dataframe, Null, parcel_structures, Null = get_parcel_geoms(PARNO)\n",
        "\n",
        "        parcel_veg = update_vegetation_neighborhood(parcel_dataframe,update=True,return_gpd =True,do_all=False)\n",
        "        parcel_veg['area'] = parcel_veg.area\n",
        "\n",
        "        veg_zone_gen_vec = []\n",
        "        if(len(parcel_dataframe)>0):\n",
        "            neighborhood_veg= parcel_veg[[not value for value in np.isin(parcel_veg['LF_FOREST'] ,  ['Developed','Water'])]]\n",
        "            veg_zone_gen_vec = [veg_zone_gen(i_samp=i_samp,vegetation_dataframe= parcel_veg) for i_samp in range(len(parcel_veg))]\n",
        "\n",
        "        structure_zone_gen_vec = []\n",
        "        if(len(parcel_structures)>0):\n",
        "            structure_dataframe = get_structure_pareto(parcel_structures)\n",
        "            structure_zone_gen_vec = [structure_zone_gen(m,structure_dataframe) for m in range(len(structure_dataframe))]\n",
        "\n",
        "        if((len(structure_zone_gen_vec)> 0) & (len(veg_zone_gen_vec)> 0) ):\n",
        "            zone_vec = [pd.concat(structure_zone_gen_vec),pd.concat(veg_zone_gen_vec)]\n",
        "\n",
        "        if((len(structure_zone_gen_vec)== 0) & (len(veg_zone_gen_vec)> 0) ):\n",
        "            zone_vec = [pd.concat(veg_zone_gen_vec)]\n",
        "\n",
        "        if((len(structure_zone_gen_vec)> 0) & (len(veg_zone_gen_vec)== 0) ):\n",
        "            zone_vec = [pd.concat(structure_zone_gen_vec)]\n",
        "\n",
        "        if((len(structure_zone_gen_vec)== 0) & (len(veg_zone_gen_vec)== 0) ):\n",
        "            zone_vec = []\n",
        "\n",
        "        zones = pd.concat(zone_vec)\n",
        "\n",
        "\n",
        "        bounding_box = gpd.GeoDataFrame(geometry=[zones.geometry.unary_union.envelope])\n",
        "\n",
        "        #Get wkt of neighborhood of parcels\n",
        "        neighborhood_geometry_wkt = bounding_box['geometry'].to_wkt()[0]\n",
        "\n",
        "        #Query for extended neighborhood (1000m) of structures\n",
        "\n",
        "        engine = create_engine(f\"postgresql://{db_cred['user']}:{db_cred['password']}@{db_cred['host']}:{db_cred['port']}/{db_cred['database']}\")\n",
        "\n",
        "        sql_query_parcels = f\"SELECT * FROM public.parcel_veg WHERE ST_INTERSECTS(geom, ST_BUFFER('SRID=4326;{neighborhood_geometry_wkt}',0))\"\n",
        "\n",
        "        neighborhood_veg = geopandas.read_postgis(sql_query_parcels, engine)\n",
        "\n",
        "        sql_query_parcels = f\"SELECT * FROM public.california_parcels WHERE ST_INTERSECTS(geom, ST_BUFFER('SRID=4326;{neighborhood_geometry_wkt}',0))\"\n",
        "\n",
        "        region_parcels = geopandas.read_postgis(sql_query_parcels, engine)\n",
        "\n",
        "        sql_query_structures = f\"SELECT * FROM public.california_structures WHERE ST_INTERSECTS(geom, ST_BUFFER('SRID=4326;{neighborhood_geometry_wkt}',0))\"\n",
        "\n",
        "        region_structures = geopandas.read_postgis(sql_query_structures, engine)\n",
        "\n",
        "        result = [not value for value in np.isin(region_parcels['parno'], neighborhood_veg['parno'])]\n",
        "\n",
        "\n",
        "        neighborhood_veg = update_vegetation_neighborhood(region_parcels,update=True,return_gpd =True,do_all=False)\n",
        "\n",
        "        zones['parno'] = PARNO\n",
        "\n",
        "        zones = zones.to_crs(6414)\n",
        "        neighborhood_veg = neighborhood_veg.to_crs(6414)\n",
        "        region_structures = region_structures.to_crs(6414)\n",
        "        region_parcels = region_parcels.to_crs(6414)\n",
        "\n",
        "        # print(zones)\n",
        "        # zones['area'] = zones.area\n",
        "        neighborhood_veg['area'] = neighborhood_veg.area\n",
        "        region_structures['area'] = region_structures.area\n",
        "\n",
        "        neighborhood_veg_zones = geopandas.overlay(\n",
        "            zones,\n",
        "            neighborhood_veg,\n",
        "            how=\"intersection\")\n",
        "\n",
        "        region_structures['Class_str'] = 'Structure'\n",
        "        parno_region_structures = geopandas.overlay(\n",
        "            region_structures,\n",
        "            region_parcels,\n",
        "            how=\"intersection\")\n",
        "\n",
        "\n",
        "\n",
        "        neighborhood_structure_zones = geopandas.overlay(\n",
        "            zones,\n",
        "            parno_region_structures,\n",
        "            how=\"intersection\")\n",
        "\n",
        "        sample_connection = pd.concat([neighborhood_veg_zones,neighborhood_structure_zones])\n",
        "\n",
        "        # print(sample_connection)\n",
        "\n",
        "\n",
        "        sample_connection['area'] = sample_connection.area\n",
        "\n",
        "        sample_connection['embers'] = sample_connection['counts_area_r']*sample_connection['area']\n",
        "\n",
        "        # print(sample_connection)\n",
        "        # sample_connection['dist_min'] = sample_connection['values']\n",
        "        sample_connection['dist_min'] = sample_connection['values']\n",
        "        sample_connection['dist_max'] = sample_connection['values']\n",
        "\n",
        "        sample_connection['MG_2'] = sample_connection['MG_2'].fillna(0)\n",
        "        sample_connection['MG_1'] = sample_connection['MG_1'].fillna(0)\n",
        "\n",
        "        # print(sample_connection[sample_connection['Class_str_2'] == 'Structure'][['Class_str_2','Class_str_1','parno_2','parno_1','embers','area','MG_2','MG_1','area_2','area_1','dist_min','dist_max']])\n",
        "\n",
        "        sample_summary = sample_connection.groupby(['parno_2','parno_1','Class_str_2','Class_str_1']).agg({'embers': 'sum','area':'sum','MG_2':'sum','MG_1':'sum','area_2':'mean','area_1':'mean','dist_min':'min','dist_max':'max'}).reset_index()\n",
        "\n",
        "\n",
        "        sample_summary['embM2'] = sample_summary['embers'] / sample_summary['area'] # simplified the lambda function\n",
        "\n",
        "        # Get centroids of the target parcel cover by class and parno\n",
        "        parcel_cover_zones = sample_connection[['parno_2', 'Class_str_2', 'geometry']].dissolve(by=['Class_str_2', 'parno_2']).reset_index()\n",
        "        parcel_cover_zones['class_parno_centroid'] = parcel_cover_zones.centroid\n",
        "\n",
        "        # Get centroids of all parcels\n",
        "        parcel_zones = neighborhood_veg_zones[['parno_2', 'geometry']].dissolve(by=['parno_2']).reset_index()\n",
        "        parcel_zones['parcel_centroid'] = parcel_zones.centroid\n",
        "\n",
        "        # Create a dictionary of target centroids with (Class_str_2, parno_2) as key\n",
        "        targ_centroids = dict(zip(zip(parcel_cover_zones['Class_str_2'], parcel_cover_zones['parno_2']), parcel_cover_zones['class_parno_centroid']))\n",
        "\n",
        "        # Assign target centroids to each class in the summary\n",
        "        sample_summary['geom'] = sample_summary.apply(lambda row: targ_centroids.get((row['Class_str_2'], row['parno_2'])), axis=1)\n",
        "\n",
        "        sample_summary['geom'] = sample_summary['geom'].apply(lambda x: geoalchemy2.WKTElement(x.wkt, srid=6414))\n",
        "        # print(sample_summary)\n",
        "\n",
        "        # sample_summary = sample_summary.to_crs(4326)\n",
        "\n",
        "    if update:\n",
        "        # Start a transaction\n",
        "        with engine.begin() as conn:\n",
        "            print(conn)\n",
        "            # Remove rows with existing 'parno' values\n",
        "            existing_parnos = sample_summary['parno_1'].unique()\n",
        "            existing_parnos_str = ','.join(f\"'{parno_1}'\" for parno_1 in existing_parnos) # surround each parno with quotes and join them with commas\n",
        "\n",
        "            print(existing_parnos_str)\n",
        "            delete_query = text(f\"DELETE FROM embernet_kpartite WHERE parno_1 IN ({existing_parnos_str})\")\n",
        "            conn.execute(delete_query)\n",
        "\n",
        "        # sample_summary['geom'] = sample_summary['geom'].apply(lambda x: geoalchemy2.WKTElement(x.wkt, srid=4326))\n",
        "\n",
        "        engine = create_engine(f\"postgresql://{db_cred['user']}:{db_cred['password']}@{db_cred['host']}:{db_cred['port']}/{db_cred['database']}\")\n",
        "\n",
        "        sample_summary.to_sql('embernet_kpartite', engine, if_exists='append', index=False,\n",
        "                        dtype={'geom': geoalchemy2.Geometry('Point', srid= 6414)})\n",
        "        engine.dispose()\n",
        "\n",
        "    if return_gpd:\n",
        "        print(PARNO)\n",
        "        engine = create_engine(f\"postgresql://{db_cred['user']}:{db_cred['password']}@{db_cred['host']}:{db_cred['port']}/{db_cred['database']}\")\n",
        "        sql_query_parcels = f\"SELECT * FROM public.embernet_kpartite WHERE parno_1 = '{PARNO}'\"\n",
        "\n",
        "        sample_summary = geopandas.read_postgis(sql_query_parcels, engine)\n",
        "        engine.dispose()\n",
        "\n",
        "        return(sample_summary)"
      ],
      "metadata": {
        "id": "W_T1b2KnkwZn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build_network_kpartite(PARNO = '016-830-009', do_all = True, update=True, return_gpd = False)"
      ],
      "metadata": {
        "id": "nCUcBXAbkyDK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "\n",
        "num_cores = multiprocessing.cpu_count()\n",
        "\n",
        "print(f\"Number of cores: {num_cores}\")"
      ],
      "metadata": {
        "id": "tXyx81TVx2eL",
        "outputId": "b9441b78-2e3d-4df1-c5b2-972de1258f39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of cores: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = '/content/drive/MyDrive/embernet/CWOs/'\n",
        "files = os.listdir(path)\n",
        "print(files[2])\n",
        "for file_name in [files[2]]:\n",
        "    CWO_path = path+file_name\n",
        "    CWO_pd = geopandas.read_file(CWO_path)\n",
        "    CWO_pd = CWO_pd.dissolve()\n",
        "    #Query for neighborhood of parcels\n",
        "    CWO_pd_buffer = CWO_pd.to_crs(6414).buffer(3000)\n",
        "    district_wkt_4326 = CWO_pd_buffer.to_crs(4326).geometry.iloc[0].wkt\n",
        "    district_wkt_6414 = CWO_pd_buffer.to_crs(6414).geometry.iloc[0].wkt\n",
        "    db_cred = {'user': 'jifesypi',\n",
        "        'password': 'Qxds23zjkpIIU343-GGHNlxqFdD3Pdlr',\n",
        "        'host': 'mild-williams-pear.db.elephantsql.com',\n",
        "        'port': '5432',\n",
        "        'database': 'jifesypi',\n",
        "        'epsg': '4326',\n",
        "        'schema':'schema'\n",
        "        }\n",
        "\n",
        "    #Query for neighborhood of parcels\n",
        "    district_wkt = CWO_pd_buffer.geometry.iloc[0].wkt\n",
        "    engine = create_engine(f\"postgresql://{db_cred['user']}:{db_cred['password']}@{db_cred['host']}:{db_cred['port']}/{db_cred['database']}\")\n",
        "    sql_query_parcels = f\"SELECT * FROM california_parcels WHERE ST_Intersects(california_parcels.geom, ST_GeomFromText('{district_wkt_4326}',4326));\"\n",
        "\n",
        "    #Send to engine\n",
        "    community_parcels = geopandas.read_postgis(sql_query_parcels, engine)\n",
        "\n",
        "    #Send to engine\n",
        "    sql_query_parcels = f\"SELECT * FROM embernet_kpartite WHERE ST_Intersects(embernet_kpartite.geom, ST_GeomFromText('{district_wkt_6414}',6414));\"\n",
        "    ember_net_complete = geopandas.read_postgis(sql_query_parcels, engine)\n",
        "\n",
        "    # incomplete_list = [not item for item in community_parcels['parno'].isin(ember_net_complete['parno_1'])]\n",
        "\n",
        "    # community_parcels = community_parcels[incomplete_list]\n",
        "\n",
        "    # neighborhood_veg = update_vegetation_neighborhood(community_parcels,update=False,return_gpd =False,do_all=False)\n",
        "    present_values = community_parcels['parno'][community_parcels['parno'].isin(ember_net_complete['parno_1'])].unique()\n",
        "    missing_values = community_parcels['parno'][~community_parcels['parno'].isin(ember_net_complete['parno_1'])].unique()\n",
        "\n",
        "    print(f\"Values present in both columns are:\\n{len(present_values)}\")\n",
        "    print(f\"Values missing from second column are:\\n{len(missing_values)}\")\n",
        "    # print(len(community_parcels['parno']))\n",
        "    # for PARNO in parcel_dataframe['parno']:\n",
        "    def process(i):\n",
        "        try:\n",
        "            build_network_kpartite(PARNO = i, do_all = True, update=True, return_gpd = False)\n",
        "            result = None\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred for input {i}: {str(e)}\")\n",
        "            result = None\n",
        "        return result\n",
        "\n",
        "    results = Parallel(n_jobs=16)(delayed(process)(i) for i in missing_values)"
      ],
      "metadata": {
        "id": "wCWOzCqTyKIk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "527e9dab-4558-40d9-8a55-fee231dc512b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SeaRanchParcels.gpkg\n",
            "Values present in both columns are:\n",
            "0\n",
            "Values missing from second column are:\n",
            "3473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "HNURVewUPwtL",
        "outputId": "727369ae-cbff-438f-bf6e-7d90cb9f29e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}